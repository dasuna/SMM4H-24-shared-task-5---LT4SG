{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xqc48xasNxV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/My Drive/SMM4H-2024"
      ],
      "metadata": {
        "id": "lx-otEiUs_A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate\n",
        "!pip3 install emoji==0.6.0"
      ],
      "metadata": {
        "id": "IBPaMv7btCJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizerFast, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import re\n",
        "import html\n",
        "import torch"
      ],
      "metadata": {
        "id": "BKjg0IxStF5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation or test data here. The purpose is to predict classes based on saved fine-tuned models.\n",
        "val_data = pd.read_csv('SMM4H-2024-Task5-Test-Unlabeled.tsv', sep='\\t')\n",
        "# Remove the \"twitter_id\" column\n",
        "val_data = val_data.drop(columns=[\"tweet_id\"])\n",
        "\n",
        "# If your test/validation dataset is too large to be processed by existing Colab GPUs, you can split it as follows\n",
        "# val_data = val_data.iloc[9750:10000]\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    # Remove URLs\n",
        "    cleaned_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    cleaned_tweet = re.sub(r'@\\w+', '', cleaned_tweet)\n",
        "    cleaned_tweet = re.sub(r'#', '', cleaned_tweet)\n",
        "\n",
        "    # Remove unscape HTML tags\n",
        "    cleaned_tweet = html.unescape(cleaned_tweet)\n",
        "\n",
        "    # Remove extra spaces, tabs, and newlines\n",
        "    cleaned_tweet = re.sub(r'\\s+', ' ', cleaned_tweet).strip()\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "val_data[\"text\"] = val_data[\"text\"].apply(preprocess_tweet)\n",
        "print(val_data.head())"
      ],
      "metadata": {
        "id": "26qqZ4IptJDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model from the checkpoint\n",
        "model_checkpoint = \"/content/gdrive/My Drive/SMM4H-2024/smm4h_classification_700/checkpoint-5550\"  # Specify the path to your checkpoint directory\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "model = model.to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "# Preprocess the text data using your tokenizer\n",
        "encoded_data = tokenizer(\n",
        "    val_data[\"text\"].tolist(), padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=512\n",
        ")\n",
        "\n",
        "# Load the encoded data onto the device (GPU if available)\n",
        "input_ids = encoded_data[\"input_ids\"].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "attention_mask = encoded_data[\"attention_mask\"].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(next(model.parameters()).device)\n",
        "print(input_ids.device, attention_mask.device)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():  # Disable gradient calculation for efficiency during inference\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Extract predictions (assuming your model has a classification head)\n",
        "logits = outputs.logits  # logits represent the unnormalized model outputs\n",
        "predictions = torch.argmax(logits, dim=-1)  # Get the class with the highest probability\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=-1)  # Apply softmax to get probabilities for normalization\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)\n",
        "\n",
        "print(probabilities[:, 1])  # Access probabilities for class 1"
      ],
      "metadata": {
        "id": "CvaG07cYtMs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}