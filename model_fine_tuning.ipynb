{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgvh5UhWU_yQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/My Drive/SMM4H-2024"
      ],
      "metadata": {
        "id": "bNyh0S7UWtHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets evaluate accelerate\n",
        "!pip install scikit-learn\n",
        "!pip install tensorboard\n",
        "!pip3 install emoji==0.6.0"
      ],
      "metadata": {
        "id": "dJTjoX3WWwFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizerFast, DataCollatorWithPadding, pipeline, BioGptForTokenClassification, OpenAIGPTForSequenceClassification\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import evaluate\n",
        "import glob\n",
        "import numpy as np\n",
        "import spacy\n",
        "import html\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import set_seed\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_PROCS = 32\n",
        "LR = 7.21422e-06\n",
        "WEIGHT_DECAY = 0.00694763\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('SMM4H-2024-Task5-Training.tsv', sep='\\t')\n",
        "# Remove the \"twitter_id\" column\n",
        "train_data = train_data.drop(columns=[\"tweet_id\"])\n",
        "\n",
        "# Load the validation data\n",
        "val_data = pd.read_csv('SMM4H-2024-Task5-Validation.tsv', sep='\\t')\n",
        "# Remove the \"twitter_id\" column\n",
        "val_data = val_data.drop(columns=[\"tweet_id\"])\n",
        "\n",
        "# View the first few rows of the training data\n",
        "print(train_data.head())\n",
        "# View the first few rows of the validation data\n",
        "print(val_data.head())"
      ],
      "metadata": {
        "id": "lseG9HgnWyoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    # Remove URLs\n",
        "    cleaned_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    cleaned_tweet = re.sub(r'@\\w+', '', cleaned_tweet)\n",
        "    cleaned_tweet = re.sub(r'#', '', cleaned_tweet)\n",
        "\n",
        "    # Remove unscape HTML tags\n",
        "    cleaned_tweet = html.unescape(cleaned_tweet)\n",
        "\n",
        "    # Remove extra spaces, tabs, and newlines\n",
        "    cleaned_tweet = re.sub(r'\\s+', ' ', cleaned_tweet).strip()\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "train_data[\"text\"] = train_data[\"text\"].apply(preprocess_tweet)\n",
        "val_data[\"text\"] = val_data[\"text\"].apply(preprocess_tweet)\n",
        "\n",
        "# View the first few rows of the training data\n",
        "print(train_data.head())\n",
        "# View the first few rows of the validation data\n",
        "print(val_data.head())"
      ],
      "metadata": {
        "id": "4CaV1o4IW8tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"Negative\",\n",
        "    1: \"Positive\",\n",
        "}\n",
        "label2id = {\n",
        "    \"Negative\": 0,\n",
        "    \"Positive\": 1,\n",
        "}\n",
        "\n",
        "\n",
        "# Convert train_data and val_data from Pandas DataFrames into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "valid_dataset = Dataset.from_pandas(val_data)"
      ],
      "metadata": {
        "id": "R1sQtm0SXASD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this if you want to set random seed values manually\n",
        "#set_seed(1)\n",
        "\n",
        "# Load BERTweet tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
        "\n",
        "# Load BERT tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load PubMedBERT abstract only tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
        "\n",
        "# Load PubMedBERT abstract large tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract\")\n",
        "\n",
        "# Load PubMedBERT abstract + full-text tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "# Load BioBERT tokenizer\n",
        "#tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "\n",
        "# Load BioLinkBERT-base tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-base')\n",
        "\n",
        "# Load BioLinkBERT-large tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-large')\n",
        "\n",
        "# Load RoBERTa-base tokenizer\n",
        "#tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "# Load RoBERTa-large tokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n",
        "\n",
        "# Load ClinicalBERT tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
        "\n",
        "# Helper function for preprocessing.\n",
        "def preprocess_function(examples):\n",
        "    max_length = 512  # Adjust as needed\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
        "\n",
        "#tokenizing training data\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_proc=NUM_PROCS\n",
        ")\n",
        "#tokenizing validation data\n",
        "tokenized_valid = valid_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_proc=NUM_PROCS\n",
        ")\n",
        "\n",
        "# Initialize data collator.\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "L0MyQhg2XEIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load('accuracy')\n",
        "f1_score = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    #calculating the accuracy\n",
        "    #accuracy_result = accuracy.compute(predictions=predictions, references=labels)\n",
        "    #calulating the F1 score\n",
        "    f1_score_result = f1_score.compute(predictions=predictions, references=labels)\n",
        "    return f1_score_result"
      ],
      "metadata": {
        "id": "JCRp3VLwXG2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is basically for loading the model\n",
        "\"\"\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"vinai/bertweet-large\",\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-large\",\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ],
      "metadata": {
        "id": "5CoutoQgXKJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"smm4h_classification_900\",\n",
        "    #seed=123,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    report_to='tensorboard',\n",
        "    fp16=True,\n",
        "    logging_steps=500\n",
        ")\n",
        "\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    callbacks=[],\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "history = trainer.train()\n",
        "\n",
        "# Get predictions for the evaluation dataset using the best model\n",
        "eval_predictions = trainer.predict(tokenized_valid)\n",
        "\n",
        "# Access the predicted labels from the output\n",
        "predicted_labels = eval_predictions.predictions.argmax(axis=1)  # Get class indices\n",
        "\n",
        "# Access the predicted probabilities for class \"1\" (or any other class) from the output\n",
        "predicted_probabilities_class_1 = eval_predictions.predictions[:, 1]  # Assuming class \"1\" is index 1\n",
        "\n",
        "# You can now process or analyze the predicted labels for the evaluation set\n",
        "print(f\"Predicted labels for evaluation data: {predicted_labels}\")\n",
        "\n",
        "# You can now process or analyze the predicted probabilities for the evaluation set\n",
        "print(f\"Predicted probabilities for class 1 (logit values): {predicted_probabilities_class_1}\")\n",
        "\n",
        "\n",
        "# Apply softmax to convert logits to probabilities\n",
        "probabilities = np.exp(eval_predictions.predictions) / np.sum(np.exp(eval_predictions.predictions), axis=1, keepdims=True)\n",
        "\n",
        "# Now you can access the normalized probabilities for class \"1\"\n",
        "predicted_probabilities_class_1_normalized = probabilities[:, 1]\n",
        "\n",
        "# You can now process or analyze the predicted probabilities for the evaluation set\n",
        "print(f\"Predicted probabilities for class 1 (normalized values): {predicted_probabilities_class_1_normalized}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "84NAfyVAXORu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}