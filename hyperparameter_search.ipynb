{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDgyvdLmjPB_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/My Drive/SMM4H-2024"
      ],
      "metadata": {
        "id": "Lzdf6imojrQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets evaluate accelerate\n",
        "!pip install scikit-learn\n",
        "!pip install tensorboard\n",
        "!pip install ray[tune]\n",
        "!pip install -U hyperopt\n",
        "!pip3 install emoji==0.6.0"
      ],
      "metadata": {
        "id": "6ipABEjbjvIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizerFast, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import evaluate\n",
        "import glob\n",
        "import numpy as np\n",
        "import html\n",
        "from ray import tune\n",
        "import random\n",
        "from ray.tune.stopper import TrialPlateauStopper\n",
        "\n",
        "NUM_PROCS = 32\n",
        "\n",
        "# Load the training data\n",
        "train_data = pd.read_csv('SMM4H-2024-Task5-Training.tsv', sep='\\t')\n",
        "# Remove the \"twitter_id\" column\n",
        "train_data = train_data.drop(columns=[\"tweet_id\"])\n",
        "\n",
        "# Load the validation data\n",
        "val_data = pd.read_csv('SMM4H-2024-Task5-Validation.tsv', sep='\\t')\n",
        "# Remove the \"twitter_id\" column\n",
        "val_data = val_data.drop(columns=[\"tweet_id\"])\n",
        "\n",
        "# View the first few rows of the training data\n",
        "print(train_data.head())\n",
        "# View the first few rows of the validation data\n",
        "print(val_data.head())"
      ],
      "metadata": {
        "id": "tI1YEzqAjy1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing function\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    # Remove URLs\n",
        "    cleaned_tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    cleaned_tweet = re.sub(r'@\\w+', '', cleaned_tweet)\n",
        "    cleaned_tweet = re.sub(r'#', '', cleaned_tweet)\n",
        "\n",
        "    # Remove unscape HTML tags\n",
        "    cleaned_tweet = html.unescape(cleaned_tweet)\n",
        "\n",
        "    # Remove extra spaces, tabs, and newlines\n",
        "    cleaned_tweet = re.sub(r'\\s+', ' ', cleaned_tweet).strip()\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "train_data[\"text\"] = train_data[\"text\"].apply(preprocess_tweet)\n",
        "val_data[\"text\"] = val_data[\"text\"].apply(preprocess_tweet)\n",
        "\n",
        "# View the first few rows of the training data\n",
        "print(train_data.head())\n",
        "# View the first few rows of the validation data\n",
        "print(val_data.head())"
      ],
      "metadata": {
        "id": "1S0fe_Lqj3Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"Negative\",\n",
        "    1: \"Positive\",\n",
        "}\n",
        "label2id = {\n",
        "    \"Negative\": 0,\n",
        "    \"Positive\": 1,\n",
        "}\n",
        "\n",
        "\n",
        "# Convert train_data and val_data pandas DataFrames into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "valid_dataset = Dataset.from_pandas(val_data)"
      ],
      "metadata": {
        "id": "bukwvlHuj7ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERTweet large tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
        "\n",
        "# Load BERT tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load PubMedBERT abstract only tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
        "\n",
        "# Load PubMedBERT abstract large tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract\")\n",
        "\n",
        "# Load PubMedBERT abstract + full-text tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "# Load BioBERT tokenizer\n",
        "#tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "\n",
        "# Load BioLinkBERT-base tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-base')\n",
        "\n",
        "# Load BioLinkBERT-large tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-large')\n",
        "\n",
        "# Load RoBERTa-base tokenizer\n",
        "#tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "# Load RoBERTa-large tokenizer\n",
        "#tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n",
        "\n",
        "# Helper function for preprocessing.\n",
        "def preprocess_function(examples):\n",
        "    max_length = 512  # Adjust as needed\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
        "\n",
        "\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=NUM_PROCS\n",
        ")\n",
        "\n",
        "tokenized_valid = valid_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=NUM_PROCS\n",
        ")\n",
        "\n",
        "# Initialize data collator.\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "czP_ThOSj-rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load('accuracy')\n",
        "f1_score = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    #accuracy_result = accuracy.compute(predictions=predictions, references=labels)\n",
        "    f1_score_result = f1_score.compute(predictions=predictions, references=labels)\n",
        "    return f1_score_result"
      ],
      "metadata": {
        "id": "x90ZqvhRkB4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter space\n",
        "def ray_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": tune.loguniform(0.5e-5, 1.5e-5),\n",
        "        \"weight_decay\": tune.loguniform(0.005, 0.05),\n",
        "        \"per_device_train_batch_size\": tune.choice([8, 16]),\n",
        "        \"per_device_eval_batch_size\": tune.choice([8, 16])\n",
        "    }\n",
        "\n",
        "# change the model name here as you want\n",
        "def model_init(trial):\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        'vinai/bertweet-large', return_dict=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"test\", evaluation_strategy=\"epoch\", logging_strategy=\"steps\", save_strategy=\"steps\", save_steps=1000, disable_tqdm=True)\n",
        "\n",
        "# Defining an instance of Hugging Face Trainer Class\n",
        "trainer = Trainer(\n",
        "    model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    model_init=model_init,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Performing hyperparameters search\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    hp_space=ray_hp_space,\n",
        "    n_trials=10\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "m55pZfZKkE9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}